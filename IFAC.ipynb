{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics-based RL\n",
      "Eval num_timesteps=1000, episode_reward=-2951.34 +/- 26.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2957.67 +/- 20.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-875.91 +/- 58.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-905.45 +/- 40.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1141.70 +/- 20.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1125.10 +/- 34.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-872.19 +/- 34.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-856.76 +/- 26.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-996.51 +/- 16.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-953.97 +/- 22.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-981.20 +/- 11.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-993.42 +/- 40.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-983.12 +/- 26.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-943.68 +/- 46.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-807.70 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-806.69 +/- 33.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-828.22 +/- 24.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-839.36 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-895.88 +/- 54.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-896.88 +/- 16.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-800.95 +/- 23.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-810.84 +/- 19.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1105.94 +/- 106.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1104.61 +/- 81.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-861.14 +/- 18.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-846.20 +/- 24.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-920.28 +/- 17.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-908.31 +/- 14.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-872.50 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-873.99 +/- 22.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-859.22 +/- 24.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-845.88 +/- 34.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-814.11 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-823.60 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-819.60 +/- 27.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-819.53 +/- 13.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-863.35 +/- 4.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-839.08 +/- 19.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-859.10 +/- 25.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-867.84 +/- 34.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-865.65 +/- 25.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-879.61 +/- 19.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-877.79 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-854.02 +/- 30.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-830.46 +/- 29.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-261.99 +/- 302.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=-155.66 +/- 378.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=-36.20 +/- 257.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=-117.17 +/- 235.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-385.63 +/- 201.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-580.28 +/- 254.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=112.97 +/- 23.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=138.00 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=125.15 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=140.66 +/- 33.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=139.09 +/- 29.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=139.30 +/- 23.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=97.56 +/- 34.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=109.30 +/- 12.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=122.36 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=122.19 +/- 26.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=183.93 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=185.12 +/- 20.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=116.79 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=124.26 +/- 23.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=97.94 +/- 17.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=120.69 +/- 22.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=111.61 +/- 23.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=98.25 +/- 7.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=78.59 +/- 9.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=88.38 +/- 19.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=96.55 +/- 29.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=116.26 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=108.26 +/- 26.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=130.10 +/- 30.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=140.43 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=141.78 +/- 21.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=52.84 +/- 31.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=63.37 +/- 19.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=101.25 +/- 24.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=83.45 +/- 17.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=134.80 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=101.22 +/- 30.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=155.36 +/- 26.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=132.56 +/- 23.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=141.10 +/- 22.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=147.40 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=135.77 +/- 26.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=140.88 +/- 35.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=117.57 +/- 35.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=120.56 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=118.22 +/- 22.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=144.55 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=124.56 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=129.36 +/- 16.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=139.44 +/- 20.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=152.63 +/- 27.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=159.89 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=158.34 +/- 14.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=170.90 +/- 13.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=152.03 +/- 22.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=176.26 +/- 10.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=182.04 +/- 13.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=191.41 +/- 25.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=162.24 +/- 21.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=186.73 +/- 34.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=145.13 +/- 29.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=147.80 +/- 20.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=140.17 +/- 23.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=128.42 +/- 29.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=146.86 +/- 15.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=182.08 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=193.10 +/- 10.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=168.73 +/- 36.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=140.46 +/- 22.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=149.71 +/- 27.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=160.56 +/- 19.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=140.56 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=147.51 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=174.92 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=155.88 +/- 28.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=162.42 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=160.39 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=194.09 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=81.56 +/- 208.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=172.13 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=191.18 +/- 12.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=190.99 +/- 12.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=4.22 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=80.67 +/- 206.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=82.04 +/- 194.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=139.64 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=118.21 +/- 30.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=137.96 +/- 23.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=153.36 +/- 28.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=152.15 +/- 30.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=147.23 +/- 16.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=129.55 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=144.38 +/- 27.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=162.32 +/- 25.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=173.49 +/- 26.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=176.62 +/- 12.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=169.90 +/- 14.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=162.09 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=165.34 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=166.36 +/- 15.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=152.90 +/- 32.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=166.16 +/- 13.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=172.12 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=164.86 +/- 34.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=175.53 +/- 21.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=169.42 +/- 14.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=166.54 +/- 23.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=191.73 +/- 15.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=154.60 +/- 16.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=172.04 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=182.11 +/- 37.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=156.71 +/- 8.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=165.14 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=149.23 +/- 25.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=164.13 +/- 25.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=182.83 +/- 16.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=165.81 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=186.46 +/- 18.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=167.25 +/- 9.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=183.20 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=158.32 +/- 24.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=158.57 +/- 23.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=166.84 +/- 37.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=163.16 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=163.34 +/- 29.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=172.75 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=189.32 +/- 15.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=191.38 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=170.11 +/- 20.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=199.47 +/- 10.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=177000, episode_reward=203.54 +/- 18.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=192.30 +/- 12.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=198.28 +/- 19.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=98.60 +/- 185.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=99.33 +/- 208.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=203.21 +/- 30.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=190.24 +/- 7.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=176.38 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=182.35 +/- 15.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=173.66 +/- 14.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=212.74 +/- 12.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=205.02 +/- 21.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=93.48 +/- 209.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=93.82 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=174.35 +/- 12.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=172.13 +/- 27.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=184.45 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=180.02 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=181.49 +/- 13.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=169.96 +/- 12.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=177.33 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=189.49 +/- 2.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=180.32 +/- 26.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=174.81 +/- 8.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=184.89 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=187.70 +/- 19.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=176.52 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=140.64 +/- 24.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=171.54 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=183.50 +/- 8.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=170.78 +/- 17.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=192.03 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=187.16 +/- 13.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=190.93 +/- 22.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=192.07 +/- 11.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=187.02 +/- 27.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=179.55 +/- 6.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=178.18 +/- 11.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=175.71 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=201.76 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=192.16 +/- 13.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=200.75 +/- 18.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=203.62 +/- 15.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=184.58 +/- 12.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=174.55 +/- 21.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=193.73 +/- 17.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=189.07 +/- 8.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=209.93 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=197.60 +/- 19.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=200.73 +/- 29.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=191.98 +/- 11.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=168.79 +/- 24.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=181.01 +/- 21.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=191.60 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=174.31 +/- 17.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=203.36 +/- 28.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=198.03 +/- 5.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=194.59 +/- 22.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=177.16 +/- 7.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=180.30 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=194.80 +/- 12.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=196.68 +/- 17.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=191.64 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=185.39 +/- 22.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=204.25 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=205.58 +/- 24.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=212.05 +/- 23.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=203.40 +/- 9.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=212.99 +/- 23.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=246000, episode_reward=203.23 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=213.75 +/- 14.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=248000, episode_reward=198.14 +/- 15.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=202.89 +/- 16.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=193.21 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=200.89 +/- 12.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=217.27 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=205.36 +/- 20.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=131.16 +/- 194.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=219.41 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=256000, episode_reward=117.63 +/- 199.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=126.20 +/- 200.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=218.84 +/- 31.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=234.40 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=260000, episode_reward=225.06 +/- 29.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=209.01 +/- 12.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=211.26 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=209.22 +/- 10.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=107.30 +/- 183.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=115.84 +/- 189.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=210.48 +/- 8.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=217.82 +/- 10.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=199.22 +/- 20.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=199.61 +/- 19.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=198.11 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=200.71 +/- 8.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=207.01 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=195.19 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=199.91 +/- 7.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=96.62 +/- 190.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=103.80 +/- 189.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=203.95 +/- 18.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=185.52 +/- 22.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=195.61 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=199.26 +/- 7.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=207.02 +/- 13.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=203.95 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=99.11 +/- 206.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=206.46 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=211.11 +/- 23.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=215.63 +/- 20.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=230.63 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=213.16 +/- 13.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=194.94 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=194.43 +/- 27.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=217.28 +/- 8.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=217.34 +/- 11.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=204.55 +/- 11.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=208.58 +/- 18.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=222.11 +/- 15.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=200.17 +/- 17.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=218.60 +/- 28.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=202.30 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=224.08 +/- 20.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=205.01 +/- 13.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=211.58 +/- 23.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=208.89 +/- 14.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=188.63 +/- 16.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=202.38 +/- 13.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=203.50 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=203.55 +/- 13.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=203.59 +/- 20.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=219.49 +/- 17.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=206.07 +/- 29.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=189.44 +/- 24.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=208.05 +/- 18.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=211.31 +/- 12.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=214.83 +/- 16.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=190.26 +/- 12.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=202.50 +/- 26.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=211.49 +/- 19.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=202.58 +/- 15.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=13.86 +/- 238.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=241.11 +/- 21.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=110.93 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=215.22 +/- 10.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=200.10 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=209.09 +/- 26.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=230.64 +/- 4.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=222.01 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=206.82 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=212.11 +/- 15.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=199.88 +/- 21.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=210.67 +/- 33.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=225.40 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=228.25 +/- 19.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=206.89 +/- 18.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=219.25 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=207.62 +/- 13.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=219.96 +/- 11.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=212.39 +/- 25.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=221.67 +/- 15.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=129.33 +/- 203.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=232.86 +/- 14.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=216.87 +/- 15.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=214.39 +/- 10.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=228.21 +/- 13.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=228.66 +/- 16.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=223.04 +/- 12.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=229.30 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=235.74 +/- 19.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=217.04 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=213.19 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=223.26 +/- 11.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=225.31 +/- 9.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=229.95 +/- 10.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=211.34 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=215.29 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=211.55 +/- 18.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=213.46 +/- 9.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=217.18 +/- 10.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=122.99 +/- 191.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=214.40 +/- 15.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=188.21 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=207.73 +/- 15.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=104.83 +/- 195.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=191.70 +/- 22.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=215.14 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=201.55 +/- 12.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=208.68 +/- 13.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=218.21 +/- 28.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=180.09 +/- 6.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=184.35 +/- 12.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=195.65 +/- 23.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=189.44 +/- 18.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=111.32 +/- 200.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=201.64 +/- 26.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=199.77 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=212.01 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=197.17 +/- 20.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=213.68 +/- 16.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=204.82 +/- 9.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=207.95 +/- 20.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=202.45 +/- 13.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=214.67 +/- 25.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=200.92 +/- 14.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=117.60 +/- 209.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=210.76 +/- 12.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=210.43 +/- 22.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=215.82 +/- 20.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=219.15 +/- 14.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=212.22 +/- 16.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=221.18 +/- 13.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=211.14 +/- 9.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=217.48 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=211.47 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=202.72 +/- 13.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=217.26 +/- 7.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=210.03 +/- 28.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=97.81 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=197.09 +/- 15.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=217.43 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=222.77 +/- 12.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=199.11 +/- 15.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=198.27 +/- 22.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=200.51 +/- 17.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=204.05 +/- 8.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=205.37 +/- 10.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=211.98 +/- 9.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=212.06 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=205.59 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=222.79 +/- 26.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=194.21 +/- 15.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=22.88 +/- 248.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=202.24 +/- 14.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=119.58 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=211.81 +/- 12.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=202.80 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=206.16 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=224.54 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=117.97 +/- 189.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=210.54 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=108.06 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=206.55 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=210.37 +/- 19.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=196.71 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=207.93 +/- 13.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=204.18 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=209.30 +/- 15.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=120.58 +/- 198.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=220.04 +/- 8.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=209.15 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=213.82 +/- 12.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=208.81 +/- 12.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=221.23 +/- 7.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=221.89 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=104.74 +/- 184.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=124.48 +/- 191.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=225.30 +/- 13.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=209.91 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=127.72 +/- 184.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=223.41 +/- 21.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=234.49 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=215.93 +/- 21.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=113.49 +/- 191.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=223.81 +/- 12.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=237.64 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=226.99 +/- 11.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=214.84 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=228.53 +/- 11.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=214.48 +/- 16.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=216.12 +/- 17.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=232.59 +/- 25.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=228.49 +/- 23.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=217.41 +/- 25.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=218.63 +/- 13.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=229.18 +/- 12.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=138.46 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=220.45 +/- 26.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=231.39 +/- 9.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=224.64 +/- 25.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=229.75 +/- 27.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=142.28 +/- 197.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=228.66 +/- 16.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=224.82 +/- 11.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=219.58 +/- 11.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=214.71 +/- 20.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=225.93 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=217.60 +/- 21.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=211.83 +/- 20.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=215.29 +/- 15.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=225.71 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=207.72 +/- 8.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=215.91 +/- 21.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=216.30 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=226.19 +/- 11.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=230.44 +/- 12.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=227.80 +/- 20.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=242.90 +/- 26.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=475000, episode_reward=246.85 +/- 16.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=476000, episode_reward=228.95 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=243.25 +/- 14.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=245.18 +/- 25.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=35.72 +/- 231.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=227.42 +/- 25.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=242.44 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=239.68 +/- 26.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=223.96 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=221.60 +/- 10.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=224.30 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=240.09 +/- 18.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=228.87 +/- 10.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=239.12 +/- 14.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=143.97 +/- 184.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=128.57 +/- 196.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=154.52 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=237.97 +/- 16.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=235.86 +/- 7.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=123.14 +/- 197.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=245.85 +/- 17.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=228.85 +/- 12.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=144.89 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=233.29 +/- 26.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=249.05 +/- 10.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500000, episode_reward=230.41 +/- 31.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=142.55 +/- 193.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=242.17 +/- 17.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=239.75 +/- 18.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=126.19 +/- 199.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=228.53 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=154.41 +/- 199.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=239.72 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=233.02 +/- 14.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=236.71 +/- 20.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=222.10 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=227.95 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=236.48 +/- 35.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=46.87 +/- 239.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=245.72 +/- 17.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=239.40 +/- 22.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=230.70 +/- 20.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=235.79 +/- 27.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=238.88 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=138.86 +/- 192.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=241.09 +/- 22.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=232.40 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=249.14 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=523000, episode_reward=249.36 +/- 26.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=524000, episode_reward=259.34 +/- 18.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=525000, episode_reward=246.12 +/- 19.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=232.17 +/- 19.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=152.32 +/- 197.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=40.25 +/- 243.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=151.16 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=28.37 +/- 254.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=257.34 +/- 12.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=241.04 +/- 16.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=140.93 +/- 201.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=241.07 +/- 12.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=146.33 +/- 204.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=256.16 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=256.74 +/- 20.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=251.62 +/- 28.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=147.20 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=256.83 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=142.15 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=140.27 +/- 193.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=141.40 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=146.32 +/- 185.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=58.79 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=215.94 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=151.90 +/- 203.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=243.81 +/- 24.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=239.80 +/- 11.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=237.16 +/- 9.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=150.30 +/- 190.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=249.42 +/- 13.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=157.68 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=252.22 +/- 19.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=257.65 +/- 16.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=253.49 +/- 19.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=257.10 +/- 12.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=243.73 +/- 11.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=168.34 +/- 187.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=255.79 +/- 12.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=158.97 +/- 196.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=135.74 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=242.87 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=220.33 +/- 8.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=236.43 +/- 18.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=134.81 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=149.21 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=244.89 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=232.37 +/- 4.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=232.97 +/- 8.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=237.48 +/- 16.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=241.65 +/- 20.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=232.10 +/- 19.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=238.03 +/- 17.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=238.51 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=240.78 +/- 26.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=222.85 +/- 14.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=130.86 +/- 194.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=249.55 +/- 17.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=238.87 +/- 19.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=228.34 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=221.78 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=237.96 +/- 13.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=150.77 +/- 207.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=244.90 +/- 14.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=168.66 +/- 206.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=251.16 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=240.38 +/- 21.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=46.08 +/- 243.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=261.47 +/- 22.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=591000, episode_reward=245.41 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=233.62 +/- 41.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=261.01 +/- 16.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=151.61 +/- 194.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=146.86 +/- 181.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=240.62 +/- 28.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=241.58 +/- 21.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=250.10 +/- 10.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=256.59 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=233.52 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=243.65 +/- 30.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=244.83 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=232.59 +/- 14.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=227.76 +/- 6.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=254.75 +/- 11.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=244.83 +/- 28.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=227.84 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=119.70 +/- 188.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=231.42 +/- 16.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=233.78 +/- 16.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=149.35 +/- 199.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=245.81 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=139.04 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=255.15 +/- 8.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=153.64 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=246.84 +/- 12.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=241.48 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=159.41 +/- 205.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=150.59 +/- 188.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=239.96 +/- 12.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=265.40 +/- 14.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=622000, episode_reward=237.73 +/- 14.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=249.37 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=254.79 +/- 10.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=245.85 +/- 11.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=245.91 +/- 16.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=244.04 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=245.00 +/- 8.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=156.07 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=254.02 +/- 10.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=245.48 +/- 16.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=251.13 +/- 22.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=236.39 +/- 22.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=244.45 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=144.20 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=252.19 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=250.30 +/- 8.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=253.53 +/- 28.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=149.97 +/- 207.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=260.02 +/- 19.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=251.49 +/- 16.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=248.82 +/- 13.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=244.51 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=240.66 +/- 8.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=243.62 +/- 29.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=241.61 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=247.89 +/- 15.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=257.58 +/- 18.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=255.63 +/- 8.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=255.94 +/- 10.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=260.57 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=259.88 +/- 15.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=163.10 +/- 192.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=262.72 +/- 12.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=254.87 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=265.34 +/- 7.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=169.93 +/- 196.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=263.38 +/- 15.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=262.10 +/- 17.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=164.28 +/- 211.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=175.68 +/- 202.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=260.45 +/- 6.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=252.96 +/- 28.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=175.00 +/- 182.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=162.85 +/- 200.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=250.65 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=247.52 +/- 16.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=163.82 +/- 191.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=254.10 +/- 8.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=250.63 +/- 15.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=236.24 +/- 21.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=256.17 +/- 17.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=261.82 +/- 10.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=248.99 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=258.83 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=253.53 +/- 11.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=268.78 +/- 8.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=678000, episode_reward=253.04 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=154.28 +/- 194.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=258.14 +/- 3.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=255.78 +/- 22.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=148.43 +/- 192.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=257.96 +/- 12.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=250.38 +/- 4.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=247.11 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=254.16 +/- 18.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=251.45 +/- 16.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=143.51 +/- 207.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=269.66 +/- 8.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=690000, episode_reward=258.50 +/- 11.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=263.71 +/- 8.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=259.47 +/- 13.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=269.05 +/- 13.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=247.14 +/- 15.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=262.73 +/- 11.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=144.21 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=271.51 +/- 11.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=698000, episode_reward=244.61 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=245.27 +/- 24.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=249.40 +/- 9.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=253.23 +/- 18.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=265.18 +/- 9.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=163.04 +/- 203.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=254.02 +/- 13.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=267.44 +/- 22.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=291.06 +/- 9.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=707000, episode_reward=162.57 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=259.83 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=265.27 +/- 16.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=258.58 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=270.48 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=270.92 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=160.02 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=261.05 +/- 15.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=263.99 +/- 17.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=78.77 +/- 232.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=277.62 +/- 16.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=161.10 +/- 193.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=174.58 +/- 199.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=176.15 +/- 195.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=171.66 +/- 196.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=164.59 +/- 192.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=262.38 +/- 22.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=274.08 +/- 20.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=272.87 +/- 5.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=269.85 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=268.28 +/- 9.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=283.04 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=166.32 +/- 211.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=266.16 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=270.55 +/- 9.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=259.83 +/- 11.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=262.78 +/- 15.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=265.51 +/- 11.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=263.97 +/- 13.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=285.39 +/- 14.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=258.16 +/- 4.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=252.24 +/- 2.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=251.50 +/- 15.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=275.03 +/- 26.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=279.38 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=280.09 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=267.60 +/- 17.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=275.52 +/- 12.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=261.95 +/- 8.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=257.90 +/- 18.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=161.56 +/- 198.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=255.14 +/- 25.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=257.23 +/- 22.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=263.05 +/- 20.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=179.10 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=262.15 +/- 16.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=272.97 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=255.38 +/- 4.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=286.48 +/- 22.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=160.92 +/- 199.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=261.97 +/- 9.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=255.74 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=254.84 +/- 17.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=263.51 +/- 29.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=260.61 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=157.80 +/- 200.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=249.40 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=256.67 +/- 18.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=241.33 +/- 12.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=253.98 +/- 36.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=250.30 +/- 15.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=250.82 +/- 17.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=233.94 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=258.13 +/- 15.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=255.81 +/- 8.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=260.13 +/- 7.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=247.57 +/- 9.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=261.20 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=236.02 +/- 20.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=253.64 +/- 11.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=250.19 +/- 24.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=251.69 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=263.79 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=264.97 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=237.45 +/- 23.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=241.03 +/- 7.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=248.93 +/- 22.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=260.53 +/- 7.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=244.65 +/- 11.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=258.45 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=262.62 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=262.59 +/- 14.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=267.63 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=247.48 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=250.08 +/- 9.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=245.17 +/- 17.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=177.37 +/- 193.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=277.42 +/- 12.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=270.83 +/- 29.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=168.39 +/- 190.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=170.35 +/- 193.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=177.70 +/- 182.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=70.60 +/- 392.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=271.14 +/- 24.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=171.95 +/- 183.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=258.21 +/- 14.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=258.56 +/- 26.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=166.43 +/- 203.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=249.95 +/- 22.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=256.77 +/- 9.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=154.94 +/- 187.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=252.41 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=154.26 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=251.22 +/- 9.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=265.57 +/- 8.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=262.00 +/- 19.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=258.12 +/- 9.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=261.21 +/- 21.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=250.97 +/- 21.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=244.88 +/- 10.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=254.66 +/- 9.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=269.34 +/- 12.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=248.22 +/- 14.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=256.85 +/- 14.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=267.25 +/- 23.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=156.57 +/- 185.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=261.70 +/- 17.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=261.59 +/- 3.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=259.43 +/- 17.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=257.66 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=256.50 +/- 15.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=263.63 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=260.19 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=259.67 +/- 9.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=260.89 +/- 20.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=262.66 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=165.89 +/- 199.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=265.02 +/- 20.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=175.76 +/- 195.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=265.13 +/- 13.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=250.31 +/- 6.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=251.30 +/- 16.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=265.79 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=264.93 +/- 5.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=257.07 +/- 14.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=166.41 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=271.91 +/- 8.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=269.72 +/- 9.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=254.41 +/- 9.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=267.31 +/- 19.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=245.42 +/- 16.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=158.48 +/- 194.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=256.83 +/- 25.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=244.78 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=249.58 +/- 7.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=144.15 +/- 195.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=244.63 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=255.09 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=252.52 +/- 8.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=256.99 +/- 15.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=257.49 +/- 12.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=262.43 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=259.77 +/- 21.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=261.04 +/- 9.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=271.59 +/- 12.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=252.79 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=255.85 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=244.80 +/- 15.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=251.46 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=256.75 +/- 16.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=258.09 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=262.19 +/- 6.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=273.47 +/- 12.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=259.32 +/- 9.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=247.39 +/- 19.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=270.10 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=239.40 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=261.40 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=249.51 +/- 19.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=252.63 +/- 14.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=246.88 +/- 17.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=262.18 +/- 17.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=253.60 +/- 20.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=259.99 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=250.53 +/- 16.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=256.57 +/- 23.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=164.05 +/- 195.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=247.70 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=262.69 +/- 8.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=263.47 +/- 11.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=254.04 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=259.52 +/- 19.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=170.34 +/- 197.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=256.15 +/- 21.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=254.17 +/- 25.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=242.01 +/- 3.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=265.95 +/- 11.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=272.17 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=269.36 +/- 19.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=248.54 +/- 17.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=267.19 +/- 19.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=255.32 +/- 7.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=260.61 +/- 26.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=256.89 +/- 10.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=267.10 +/- 8.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=167.68 +/- 195.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=176.76 +/- 198.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=182.52 +/- 201.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=269.76 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=263.35 +/- 16.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=267.70 +/- 15.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=256.37 +/- 9.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=164.46 +/- 191.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=264.16 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=285.90 +/- 18.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=264.28 +/- 11.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=69.43 +/- 236.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=266.07 +/- 12.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=269.27 +/- 19.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=169.00 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=273.06 +/- 21.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=261.66 +/- 22.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=270.00 +/- 11.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=175.58 +/- 201.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=174.74 +/- 198.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=264.04 +/- 19.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=62.64 +/- 239.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=263.79 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=270.59 +/- 12.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=168.80 +/- 195.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=260.79 +/- 7.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=259.24 +/- 15.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=254.87 +/- 11.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=267.60 +/- 16.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=266.28 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=259.23 +/- 9.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=267.97 +/- 7.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=265.14 +/- 11.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=260.06 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=260.45 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=255.02 +/- 15.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=165.89 +/- 189.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=237.82 +/- 18.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=262.34 +/- 17.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=252.90 +/- 10.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=257.21 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=259.59 +/- 20.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=164.50 +/- 202.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=280.52 +/- 16.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=267.90 +/- 18.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=170.02 +/- 201.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=273.04 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=259.60 +/- 19.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=271.95 +/- 12.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=144.97 +/- 190.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=258.05 +/- 9.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=169.73 +/- 194.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=251.73 +/- 17.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=243.17 +/- 12.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=255.49 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=252.89 +/- 14.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=284.49 +/- 23.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=263.96 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=171.67 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=160.24 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=151.61 +/- 183.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=278.60 +/- 12.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=265.02 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=277.24 +/- 34.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=249.10 +/- 18.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=257.86 +/- 27.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=248.78 +/- 17.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=251.12 +/- 7.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=264.12 +/- 2.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=155.48 +/- 201.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=256.84 +/- 15.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=258.89 +/- 12.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=259.88 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=269.25 +/- 10.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=264.31 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=262.16 +/- 9.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=272.24 +/- 13.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=262.01 +/- 19.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=262.39 +/- 15.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=257.30 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=270.11 +/- 5.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=265.12 +/- 9.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=266.30 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=262.54 +/- 14.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=257.70 +/- 23.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=262.26 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=273.43 +/- 6.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=262.36 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=254.98 +/- 27.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=261.65 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=274.80 +/- 15.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=262.91 +/- 17.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=164.72 +/- 197.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=284.54 +/- 18.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=262.82 +/- 19.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=264.74 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=259.08 +/- 23.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=254.95 +/- 14.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=274.11 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=247.01 +/- 15.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=261.55 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=256.58 +/- 27.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=256.85 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=254.74 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=250.47 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=171.31 +/- 206.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=260.60 +/- 19.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=263.51 +/- 21.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=245.29 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=246.79 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=261.70 +/- 10.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=243.54 +/- 10.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=262.44 +/- 19.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=258.52 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=252.02 +/- 10.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=250.66 +/- 21.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=253.90 +/- 12.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=253.75 +/- 13.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=151.34 +/- 201.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=258.99 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=262.39 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=261.99 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=243.39 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=260.28 +/- 5.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=160.23 +/- 195.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=62.94 +/- 236.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=177.03 +/- 193.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=248.15 +/- 12.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=244.89 +/- 11.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=169.10 +/- 185.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=254.82 +/- 24.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=254.09 +/- 10.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=266.31 +/- 12.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=256.62 +/- 16.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=262.82 +/- 12.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=271.22 +/- 26.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=251.67 +/- 11.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=270.66 +/- 8.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=268.62 +/- 16.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=273.81 +/- 18.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=160.78 +/- 195.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=274.60 +/- 12.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=271.62 +/- 25.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=259.16 +/- 12.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=269.95 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=254.86 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=260.80 +/- 8.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=255.71 +/- 9.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=253.61 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=248.67 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=254.46 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=264.49 +/- 22.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=259.36 +/- 11.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=267.47 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=266.31 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=250.71 +/- 12.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=265.49 +/- 27.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=248.08 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=249.70 +/- 11.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=241.58 +/- 14.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=146.25 +/- 198.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=255.91 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=240.46 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=247.43 +/- 10.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=265.31 +/- 24.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=254.23 +/- 19.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=242.13 +/- 12.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=253.05 +/- 12.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=247.14 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=244.89 +/- 24.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=259.56 +/- 12.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=257.14 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=248.79 +/- 5.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=252.31 +/- 6.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=253.33 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=256.97 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=241.66 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=247.17 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=259.55 +/- 24.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=250.16 +/- 13.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=246.06 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=257.89 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=236.48 +/- 20.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=249.35 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=154.08 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=240.15 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=260.76 +/- 27.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=233.35 +/- 6.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=255.04 +/- 27.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=244.55 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=255.78 +/- 17.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=261.36 +/- 15.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=254.10 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=252.44 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=244.31 +/- 16.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=157.18 +/- 205.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=257.59 +/- 22.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=251.26 +/- 13.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=254.98 +/- 7.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=162.07 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=258.71 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=152.93 +/- 197.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=134.74 +/- 182.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=250.87 +/- 14.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=234.33 +/- 12.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=243.48 +/- 19.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=146.55 +/- 189.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=245.67 +/- 17.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=248.55 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=246.10 +/- 19.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=250.93 +/- 24.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=252.18 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=244.95 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=231.69 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=241.55 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=245.61 +/- 23.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=252.72 +/- 17.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=269.07 +/- 20.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=243.96 +/- 20.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=237.29 +/- 18.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=251.09 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=248.39 +/- 10.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=142.55 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=248.19 +/- 17.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=245.45 +/- 9.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=238.15 +/- 14.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=233.70 +/- 14.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=153.13 +/- 201.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=252.81 +/- 8.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=253.71 +/- 6.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=251.90 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=131.08 +/- 191.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=256.18 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=228.03 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=236.30 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=243.99 +/- 8.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=244.85 +/- 17.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=149.46 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=155.53 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=249.98 +/- 8.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=251.90 +/- 14.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=167.10 +/- 189.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=246.58 +/- 18.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=151.48 +/- 197.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=140.88 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=248.55 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=254.21 +/- 20.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=167.07 +/- 187.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=257.59 +/- 16.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=258.05 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=260.79 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=260.82 +/- 9.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=63.68 +/- 250.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=253.15 +/- 17.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=147.39 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=245.48 +/- 14.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=245.66 +/- 16.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=254.83 +/- 11.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=243.85 +/- 10.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=158.22 +/- 197.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=236.85 +/- 16.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=250.98 +/- 21.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=161.07 +/- 197.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=257.74 +/- 24.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=147.85 +/- 187.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=248.29 +/- 7.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=250.08 +/- 18.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=233.58 +/- 12.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=251.05 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=250.66 +/- 10.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=145.29 +/- 175.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=241.55 +/- 25.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=251.51 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=149.35 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=143.97 +/- 180.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=154.77 +/- 190.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=244.60 +/- 19.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=254.43 +/- 8.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=247.87 +/- 7.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=261.78 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=246.45 +/- 14.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=241.77 +/- 15.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=253.78 +/- 9.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=250.05 +/- 17.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=230.74 +/- 9.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=239.75 +/- 11.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=238.53 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=224.18 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=234.75 +/- 17.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=235.07 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=237.55 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=228.66 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=239.70 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=235.79 +/- 10.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=237.76 +/- 12.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=232.47 +/- 11.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=237.60 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=234.31 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=231.46 +/- 16.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=239.52 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=240.32 +/- 21.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=242.29 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=243.50 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=142.06 +/- 188.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=255.58 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=247.60 +/- 14.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=225.24 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=254.32 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=256.55 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=244.49 +/- 15.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=215.38 +/- 17.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=229.69 +/- 18.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=210.77 +/- 23.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=237.06 +/- 10.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=222.03 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=238.07 +/- 18.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=146.02 +/- 192.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=257.98 +/- 23.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=234.79 +/- 17.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=241.11 +/- 14.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=250.85 +/- 21.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=234.71 +/- 19.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=227.60 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=238.83 +/- 14.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=249.75 +/- 18.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=234.85 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=221.12 +/- 7.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=231.48 +/- 13.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=244.77 +/- 15.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=239.22 +/- 14.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=246.27 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=235.32 +/- 13.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=232.73 +/- 15.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=245.33 +/- 9.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=234.73 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=151.64 +/- 200.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=231.54 +/- 10.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=242.35 +/- 21.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=247.42 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=245.71 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=237.42 +/- 19.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=245.81 +/- 13.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=272.44 +/- 10.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=145.94 +/- 203.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=131.53 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=240.72 +/- 11.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=250.96 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=248.06 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=236.94 +/- 10.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=238.10 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=239.39 +/- 20.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=244.64 +/- 7.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=249.80 +/- 14.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=252.87 +/- 14.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=38.98 +/- 231.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=255.00 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=248.67 +/- 13.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=248.26 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=253.73 +/- 18.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=239.37 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=241.29 +/- 13.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=238.04 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=248.76 +/- 20.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=238.85 +/- 12.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=242.07 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=246.61 +/- 11.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=241.58 +/- 26.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=248.48 +/- 12.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=239.38 +/- 9.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=238.28 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=241.86 +/- 13.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=241.25 +/- 8.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=255.32 +/- 12.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=231.11 +/- 16.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=222.36 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=233.47 +/- 11.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=227.35 +/- 12.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=230.58 +/- 8.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=238.09 +/- 18.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=227.10 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=232.35 +/- 15.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=233.19 +/- 6.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=231.35 +/- 19.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=213.51 +/- 15.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=228.28 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=228.46 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=241.65 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=239.06 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=228.85 +/- 13.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=244.40 +/- 11.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=235.59 +/- 18.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=234.77 +/- 14.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=242.62 +/- 15.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=230.05 +/- 21.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=236.39 +/- 10.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=232.14 +/- 7.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=239.60 +/- 20.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=254.57 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=128.42 +/- 199.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=139.66 +/- 197.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=235.83 +/- 12.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=248.54 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=254.44 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=242.22 +/- 6.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=240.42 +/- 21.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=239.58 +/- 12.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=244.86 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=251.36 +/- 6.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=236.74 +/- 6.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=251.39 +/- 17.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=225.72 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=236.97 +/- 13.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=251.76 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=244.22 +/- 11.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=257.05 +/- 6.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=251.46 +/- 9.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=255.22 +/- 10.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=242.04 +/- 13.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=245.73 +/- 8.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=240.92 +/- 14.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=250.81 +/- 23.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=252.18 +/- 11.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=245.91 +/- 9.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=251.67 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=249.55 +/- 23.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=150.09 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=241.62 +/- 17.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=245.48 +/- 15.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=248.65 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=268.84 +/- 13.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=248.38 +/- 15.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=164.25 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=260.14 +/- 12.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=161.34 +/- 192.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=264.78 +/- 21.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=70.15 +/- 237.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=253.52 +/- 12.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=262.01 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=246.01 +/- 11.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=141.27 +/- 193.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=245.06 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=233.34 +/- 15.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=251.53 +/- 10.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=264.50 +/- 18.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=242.72 +/- 12.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=252.43 +/- 14.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=253.73 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=249.41 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=236.00 +/- 11.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=235.54 +/- 7.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=245.53 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=250.42 +/- 18.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=242.45 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=243.43 +/- 13.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=255.54 +/- 9.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=246.68 +/- 12.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=244.96 +/- 9.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=164.86 +/- 202.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=227.79 +/- 16.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=250.41 +/- 19.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=250.82 +/- 9.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=254.32 +/- 8.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=152.32 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=249.16 +/- 17.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=243.95 +/- 12.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=251.15 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=257.35 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=248.08 +/- 12.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=251.53 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=248.96 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=258.73 +/- 13.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=249.76 +/- 15.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=239.65 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=248.08 +/- 11.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=229.81 +/- 18.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=250.67 +/- 24.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=252.84 +/- 13.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=240.21 +/- 27.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=251.74 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=252.43 +/- 13.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=241.58 +/- 11.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=245.32 +/- 7.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=249.06 +/- 12.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=248.03 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=240.31 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=140.83 +/- 189.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=153.19 +/- 192.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=235.23 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=230.09 +/- 13.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=253.59 +/- 8.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=259.57 +/- 7.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=251.13 +/- 6.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=255.52 +/- 16.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=249.33 +/- 10.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=237.59 +/- 16.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=54.75 +/- 242.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=256.36 +/- 13.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=253.80 +/- 9.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=252.14 +/- 21.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=248.02 +/- 9.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=231.05 +/- 15.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=247.42 +/- 15.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=267.61 +/- 10.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=66.04 +/- 235.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=241.54 +/- 19.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=250.45 +/- 19.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=50.00 +/- 239.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=72.31 +/- 243.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=245.77 +/- 18.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=145.36 +/- 194.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=265.42 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=256.85 +/- 7.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=241.14 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=56.08 +/- 393.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=251.68 +/- 12.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=157.92 +/- 195.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=252.94 +/- 10.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=243.13 +/- 9.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=251.13 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=155.00 +/- 200.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=253.25 +/- 9.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=244.75 +/- 5.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=247.99 +/- 13.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=244.05 +/- 26.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=242.81 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=159.07 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=251.08 +/- 24.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=254.32 +/- 23.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=248.80 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=243.58 +/- 16.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=255.19 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=263.13 +/- 16.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=249.88 +/- 19.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=256.32 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=255.61 +/- 11.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=265.53 +/- 14.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=261.66 +/- 12.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=63.82 +/- 252.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=249.56 +/- 18.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=156.86 +/- 191.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=250.64 +/- 13.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=164.13 +/- 191.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=252.64 +/- 9.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=255.82 +/- 10.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=253.92 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=264.22 +/- 13.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=154.81 +/- 187.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=156.52 +/- 196.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=235.63 +/- 21.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=247.47 +/- 11.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=257.75 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=269.44 +/- 16.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=250.27 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=264.36 +/- 16.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=278.62 +/- 14.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=253.54 +/- 18.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=260.64 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=260.58 +/- 13.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=259.53 +/- 28.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=252.70 +/- 8.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=241.76 +/- 11.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=259.96 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=265.36 +/- 17.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=253.86 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=272.81 +/- 19.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=154.79 +/- 195.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=159.50 +/- 205.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=258.04 +/- 19.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=261.40 +/- 9.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=249.52 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=261.17 +/- 11.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=250.10 +/- 7.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=150.64 +/- 195.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=253.03 +/- 8.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=255.53 +/- 8.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=259.31 +/- 10.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=277.03 +/- 11.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=266.63 +/- 21.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=270.72 +/- 13.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=268.78 +/- 11.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=256.61 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=263.12 +/- 17.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=167.08 +/- 198.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=267.59 +/- 11.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=240.98 +/- 19.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=140.35 +/- 192.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=248.49 +/- 9.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=245.84 +/- 32.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=257.94 +/- 15.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=271.73 +/- 15.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=281.89 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=169.40 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=173.92 +/- 197.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=259.79 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=264.31 +/- 8.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=258.53 +/- 13.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=263.04 +/- 7.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=245.74 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=262.96 +/- 12.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=167.33 +/- 196.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=265.50 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=255.21 +/- 10.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=62.06 +/- 227.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=258.45 +/- 15.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=259.19 +/- 8.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=264.16 +/- 11.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=159.11 +/- 195.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=262.94 +/- 14.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=243.29 +/- 10.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=258.13 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=252.13 +/- 9.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=253.11 +/- 13.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=157.65 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=248.63 +/- 17.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=275.38 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=261.36 +/- 30.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=265.67 +/- 9.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=237.90 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=268.74 +/- 20.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=176.04 +/- 196.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=263.17 +/- 9.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=256.70 +/- 16.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=257.76 +/- 11.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=244.90 +/- 10.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=245.51 +/- 19.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=256.71 +/- 21.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=259.72 +/- 11.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=249.76 +/- 4.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=246.13 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=256.56 +/- 25.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=257.99 +/- 11.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=237.41 +/- 15.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=254.78 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=261.94 +/- 8.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=251.47 +/- 19.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=154.10 +/- 189.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=251.38 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=153.96 +/- 200.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=248.61 +/- 14.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=243.80 +/- 33.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=249.40 +/- 26.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=249.17 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=244.65 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=244.48 +/- 21.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=257.63 +/- 9.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=264.90 +/- 19.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=270.71 +/- 16.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=245.31 +/- 19.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=263.98 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=255.52 +/- 20.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=257.37 +/- 18.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=160.89 +/- 185.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=262.09 +/- 18.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=254.63 +/- 4.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=257.38 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=251.14 +/- 21.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=261.82 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=148.67 +/- 212.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=261.39 +/- 27.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=162.97 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=259.74 +/- 12.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=255.22 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=259.41 +/- 11.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=260.28 +/- 15.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=251.39 +/- 18.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=261.51 +/- 14.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=258.62 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=267.90 +/- 10.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=277.35 +/- 18.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=255.85 +/- 23.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=274.96 +/- 22.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=259.75 +/- 16.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=251.96 +/- 15.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=266.34 +/- 19.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=169.78 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=252.37 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=260.53 +/- 15.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=261.80 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=259.22 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=176.04 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=174.55 +/- 192.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=262.40 +/- 11.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=255.91 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=253.99 +/- 22.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=253.02 +/- 24.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=145.43 +/- 204.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=148.22 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=262.96 +/- 18.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=250.27 +/- 15.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=255.28 +/- 10.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=258.05 +/- 13.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=256.39 +/- 15.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=259.22 +/- 21.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=257.70 +/- 18.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=262.70 +/- 11.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=241.18 +/- 16.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=269.01 +/- 18.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=264.36 +/- 25.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=245.11 +/- 8.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=258.40 +/- 13.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=164.91 +/- 203.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=270.12 +/- 14.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=261.16 +/- 19.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=252.61 +/- 8.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=159.92 +/- 183.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=267.81 +/- 15.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=259.83 +/- 12.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=258.15 +/- 12.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=239.07 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=253.93 +/- 16.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=236.48 +/- 23.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=167.34 +/- 196.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=244.37 +/- 15.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=251.19 +/- 15.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=237.57 +/- 8.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=258.18 +/- 28.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=270.22 +/- 14.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=263.11 +/- 9.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=77.85 +/- 242.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=158.59 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=263.26 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=247.71 +/- 20.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=256.20 +/- 7.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=262.76 +/- 19.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=241.39 +/- 43.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=263.55 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=154.29 +/- 199.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=249.96 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=252.07 +/- 23.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=226.45 +/- 24.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=259.60 +/- 26.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=274.53 +/- 26.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=245.17 +/- 14.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=247.61 +/- 19.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=273.37 +/- 16.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=257.19 +/- 10.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=254.06 +/- 16.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=247.57 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=251.25 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=248.20 +/- 9.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=254.71 +/- 20.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=262.56 +/- 25.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=254.68 +/- 18.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=258.56 +/- 13.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=257.11 +/- 17.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=241.98 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=230.13 +/- 14.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=255.98 +/- 10.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=146.90 +/- 204.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=252.21 +/- 23.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=247.12 +/- 11.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=245.56 +/- 14.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=252.28 +/- 12.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=257.75 +/- 13.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=264.79 +/- 25.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=266.14 +/- 14.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=272.06 +/- 16.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=269.88 +/- 18.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=268.48 +/- 9.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=261.45 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=269.39 +/- 11.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=156.47 +/- 195.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=250.29 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=151.14 +/- 184.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=251.82 +/- 23.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=251.69 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=260.60 +/- 15.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=256.90 +/- 15.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=269.61 +/- 14.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=251.01 +/- 14.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=250.66 +/- 14.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=243.51 +/- 6.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=257.09 +/- 15.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=247.72 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=264.73 +/- 12.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=258.01 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=243.61 +/- 16.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=236.28 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=255.33 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=237.27 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=248.85 +/- 7.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=257.16 +/- 4.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=256.23 +/- 10.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=252.09 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=247.15 +/- 22.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=245.32 +/- 20.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=145.55 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=260.54 +/- 13.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=153.74 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=259.97 +/- 10.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=249.83 +/- 22.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=255.84 +/- 20.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=245.87 +/- 15.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=239.81 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=238.28 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=254.09 +/- 23.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=247.94 +/- 16.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=255.34 +/- 15.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=267.21 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=255.57 +/- 8.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=149.24 +/- 192.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=158.71 +/- 211.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=243.56 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=250.43 +/- 18.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=260.42 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=259.77 +/- 17.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=247.98 +/- 12.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=141.89 +/- 192.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=254.88 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=258.44 +/- 5.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=258.73 +/- 19.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=161.83 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=254.18 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=264.62 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=253.72 +/- 10.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=253.65 +/- 16.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=258.03 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=235.54 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=252.39 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=267.43 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=252.04 +/- 22.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=256.63 +/- 11.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=265.16 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=254.53 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=257.96 +/- 9.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=265.11 +/- 20.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=253.05 +/- 19.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=260.47 +/- 10.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=261.98 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=270.41 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=260.98 +/- 35.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=255.31 +/- 18.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=49.52 +/- 391.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-34.61 +/- 233.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=268.88 +/- 15.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=271.73 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=268.10 +/- 13.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=259.93 +/- 26.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=248.39 +/- 30.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=255.39 +/- 11.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=163.49 +/- 190.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=171.07 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=256.06 +/- 11.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=253.63 +/- 16.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=266.84 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=256.95 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=266.91 +/- 20.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=260.02 +/- 10.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=259.95 +/- 13.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=256.65 +/- 25.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=254.37 +/- 18.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=236.59 +/- 20.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=258.53 +/- 22.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=258.68 +/- 17.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=259.67 +/- 11.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=252.49 +/- 12.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=239.61 +/- 27.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=283.86 +/- 23.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=252.72 +/- 10.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=265.46 +/- 8.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=246.39 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=247.52 +/- 18.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=247.27 +/- 15.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=150.53 +/- 203.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=250.99 +/- 15.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=252.92 +/- 14.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=165.31 +/- 197.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=249.68 +/- 9.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=266.00 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=253.14 +/- 22.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=253.45 +/- 12.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=162.52 +/- 202.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=277.37 +/- 15.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=244.69 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=256.26 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=255.72 +/- 21.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=159.84 +/- 185.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=255.40 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=255.60 +/- 9.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=233.77 +/- 13.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=250.72 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=246.11 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=240.97 +/- 14.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=144.35 +/- 193.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=246.50 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=263.50 +/- 5.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=260.85 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=260.33 +/- 17.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=256.59 +/- 20.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=249.95 +/- 29.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=242.77 +/- 27.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=251.00 +/- 10.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=262.09 +/- 28.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=269.30 +/- 10.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=266.42 +/- 19.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=286.40 +/- 22.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=264.03 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=243.04 +/- 10.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=149.89 +/- 197.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=263.49 +/- 12.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=159.55 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=257.58 +/- 27.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=255.80 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=248.41 +/- 11.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=277.02 +/- 18.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=164.94 +/- 192.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=263.68 +/- 17.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=146.66 +/- 196.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=271.20 +/- 9.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=68.83 +/- 235.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=155.91 +/- 187.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=162.67 +/- 196.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=256.09 +/- 6.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=163.32 +/- 195.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=158.00 +/- 197.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=172.68 +/- 192.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=272.94 +/- 16.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=262.73 +/- 7.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=255.36 +/- 16.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=161.15 +/- 195.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=272.26 +/- 8.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=261.46 +/- 19.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=158.36 +/- 205.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=252.69 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=66.11 +/- 231.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=257.27 +/- 23.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=269.31 +/- 18.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=263.03 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=254.66 +/- 7.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=255.25 +/- 6.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=152.96 +/- 200.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=258.90 +/- 20.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=271.11 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=261.60 +/- 15.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=273.05 +/- 10.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=265.14 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=262.75 +/- 8.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=264.55 +/- 7.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=249.50 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=266.32 +/- 12.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=164.38 +/- 197.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=250.20 +/- 15.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=260.95 +/- 20.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=256.42 +/- 12.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=155.30 +/- 191.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=267.67 +/- 12.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=163.99 +/- 182.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=262.82 +/- 15.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=270.68 +/- 6.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=250.31 +/- 10.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=254.04 +/- 16.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=163.60 +/- 196.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=165.13 +/- 188.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=257.16 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=272.20 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=157.16 +/- 197.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=165.51 +/- 190.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=163.51 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=161.03 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=250.14 +/- 16.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=161.53 +/- 189.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=249.61 +/- 15.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=266.31 +/- 11.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=238.12 +/- 10.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=161.37 +/- 202.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=259.09 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=-30.17 +/- 238.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=173.19 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=256.10 +/- 18.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=156.56 +/- 204.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=264.43 +/- 15.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=254.51 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=254.64 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=261.00 +/- 19.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=162.45 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=259.97 +/- 21.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=265.36 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=262.34 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=171.14 +/- 201.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=267.02 +/- 9.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=271.43 +/- 12.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=257.71 +/- 21.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=165.15 +/- 194.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=275.65 +/- 13.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=173.90 +/- 205.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=277.18 +/- 25.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=155.03 +/- 193.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=280.34 +/- 15.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=265.84 +/- 15.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=276.19 +/- 7.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=248.62 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=263.57 +/- 14.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=166.35 +/- 195.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=83.58 +/- 400.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=269.47 +/- 17.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=252.55 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=258.31 +/- 9.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=257.73 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=49.40 +/- 241.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=254.02 +/- 10.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=167.52 +/- 197.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=249.50 +/- 10.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=254.74 +/- 19.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=264.69 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=273.93 +/- 18.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=152.64 +/- 197.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=254.53 +/- 16.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=264.24 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=253.62 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=269.03 +/- 14.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=277.58 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=254.31 +/- 9.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=176.91 +/- 190.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=252.59 +/- 13.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=271.28 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=263.34 +/- 14.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=268.71 +/- 5.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=252.08 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=255.25 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=160.82 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=256.48 +/- 30.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=274.81 +/- 19.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=279.70 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=274.89 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=268.82 +/- 16.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=265.22 +/- 10.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=266.56 +/- 16.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=266.98 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=255.54 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=53.47 +/- 249.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=269.31 +/- 19.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=264.96 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=274.18 +/- 4.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=267.21 +/- 9.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=271.66 +/- 14.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=271.12 +/- 6.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=279.82 +/- 17.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=257.56 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=257.87 +/- 10.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=266.37 +/- 9.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=256.28 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=263.94 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=276.16 +/- 14.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=264.12 +/- 8.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=256.26 +/- 25.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=171.38 +/- 195.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=261.94 +/- 13.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=276.91 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=259.77 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=177.56 +/- 204.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=155.55 +/- 199.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=270.25 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=241.33 +/- 14.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=263.54 +/- 20.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=259.19 +/- 20.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=266.61 +/- 18.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=262.96 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=268.17 +/- 21.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=243.55 +/- 10.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=270.00 +/- 14.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=160.17 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=145.63 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=280.80 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=251.25 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=266.84 +/- 14.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=246.68 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=259.29 +/- 9.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=247.37 +/- 16.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=266.79 +/- 17.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=259.21 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=261.10 +/- 12.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=269.23 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=246.97 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=264.79 +/- 14.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=76.64 +/- 252.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=160.04 +/- 176.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=156.96 +/- 187.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=173.07 +/- 197.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=254.64 +/- 10.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=165.46 +/- 193.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=259.10 +/- 12.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=261.16 +/- 14.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=177.27 +/- 175.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=278.51 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=80.52 +/- 240.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=276.53 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=81.68 +/- 392.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=283.54 +/- 8.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=279.85 +/- 6.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=271.19 +/- 15.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=279.99 +/- 13.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=261.09 +/- 9.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=184.64 +/- 192.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=261.34 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=263.16 +/- 15.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=256.97 +/- 10.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=253.15 +/- 21.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=266.82 +/- 23.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=161.77 +/- 191.45\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20a0dfe9d60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "total_timesteps = 2e6\n",
    "\n",
    "# Load diagnostics/prognostics model from disk and initiate environment\n",
    "\n",
    "print('Diagnostics-based RL')\n",
    "diag_model = pickle.load(open('diagnostics/model_pn1_mn0', 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, spare_part = False, process_noise = 0.1)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/IFAC',\n",
    "                        log_path='./callback/IFAC', eval_freq=1000,\n",
    "                        deterministic=True, render=False)\n",
    "model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/IFAC/\")\n",
    "model.learn(total_timesteps=total_timesteps, tb_log_name='PPO', callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215.10500000000002, 148.42888453734332)\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "diag_model = pickle.load(open('diagnostics/model_pn1_mn0', 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, spare_part = False, process_noise = 0.1)\n",
    "model = PPO.load('./callback/IFAC/best_model', env = env)\n",
    "\n",
    "# Evaluate the agent\n",
    "print(evaluate_policy(model, model.get_env(), n_eval_episodes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  3.0\n",
      "The average sum of inventory per episode is:  79.0\n",
      "The average reward per episode is:  271.0\n",
      "The average upper bound per episode is:  406.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "import gym\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "diag_model = pickle.load(open('diagnostics/model_pn1_mn0', 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, spare_part = False, process_noise = 0.1)\n",
    "model = PPO.load('./callback/IFAC/best_model', env = env)\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==5)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations) \n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_df.to_excel(\"visuals/IFAC/episode1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  2.0\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  73.75\n",
      "The average reward per episode is:  -768.58\n",
      "The average upper bound per episode is:  400.42\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', spare_part = False)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:   \n",
    "        action = round(obs[1])\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==5)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  31\n",
      "The average number of reactive maintenance interventions per episode is:  0.02\n",
      "The average number of preventive maintenance interventions per episode is:  2.98\n",
      "The average sum of inventory per episode is:  73.49\n",
      "The average reward per episode is:  161.86\n",
      "The average upper bound per episode is:  402.65\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIb ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', spare_part = False)\n",
    "#kay = range(0,7)\n",
    "kay = range (31,32)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "for k in kay:\n",
    "    mtbf = k\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame([[0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[2], 0, done, obs[1]])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # At period of mtbf: maintain\n",
    "            if env.scheduled_maintenance_counter == mtbf:\n",
    "                action = 5\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = round(obs[1])\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[2], reward, done, obs[1]])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'reward', 'done', 'next_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==5)\n",
    "        # Calculate mean time between failures\n",
    "        # Cut df after last breakdown\n",
    "        eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", mtbf)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "    print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "    print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "    print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
